@phdthesis{deuschle2019,
    author = {William J. Deuschle},
    title = {Undergraduate Fundamentals of Machine Learning},
    year = {2019},
    school = {Harvard College},
    type = {Bachelor's thesis},
    url = {https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37364585},
}


@article{kushawahAndYadav2016,
    author = { Yogiraj Singh Kushawah, Ashish Mohan Yadav },
    title = { A Survey on Unsupervised Clustering Algorithm based on K-Means Clustering },
    journal = { International Journal of Computer Applications },
    issue_date = { Dec 2016 },
    volume = { 156 },
    number = { 8 },
    month = { Dec },
    year = { 2016 },
    issn = { 0975-8887 },
    pages = { 6-9 },
    numpages = {9},
    url = { https://ijcaonline.org/archives/volume156/number8/26727-2016912481/ },
    doi = { 10.5120/ijca2016912481 },
    publisher = {Foundation of Computer Science (FCS), NY, USA},
    address = {New York, USA}
}

% TODO: Source in eglish?

@misc{IDC_Statista_2024,
    author = {IDC and Statista},
    title = {Volumen der weltweit erstellten, erfassten, kopierten und konsumierten Daten von 2010 bis 2028},
    year = {2024},
    month = {Mai},
    day = {31},
    url = {https://de.statista.com/statistik/daten/studie/1420298/umfrage/prognose-weltweites-erstelltes-datenvolumen/},
    note = {[Online; last accessed on March 15, 2025]}
}

@article{Jain2010651,
    title = {Data clustering: 50 years beyond K-means},
    journal = {Pattern Recognition Letters},
    volume = {31},
    number = {8},
    pages = {651-666},
    year = {2010},
    issn = {0167-8655},
    doi = {https://doi.org/10.1016/j.patrec.2009.09.011},
    url = {https://www.sciencedirect.com/science/article/pii/S0167865509002323},
    author = {Anil K. Jain},
    keywords = {Data clustering, User’s dilemma, Historical developments, Perspectives on clustering, King-Sun Fu prize},
    abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.}
}

@article{Ezugwu2022104743,
    title = {A comprehensive survey of clustering algorithms: State-of-the-art machine learning applications, taxonomy, challenges, and future research prospects},
    journal = {Engineering Applications of Artificial Intelligence},
    volume = {110},
    pages = {104743},
    year = {2022},
    issn = {0952-1976},
    doi = {https://doi.org/10.1016/j.engappai.2022.104743},
    url = {https://www.sciencedirect.com/science/article/pii/S095219762200046X},
    author = {Absalom E. Ezugwu and Abiodun M. Ikotun and Olaide O. Oyelade and Laith Abualigah and Jeffery O. Agushaka and Christopher I. Eke and Andronicus A. Akinyelu},
    keywords = {Clustering, Clustering algorithms, partitioning, Data mining, Hierarchical clustering, Automatic clustering, K-Means, Optimization algorithms, Machine learning, Unsupervised learning, Supervised learning},
    abstract = {Clustering is an essential tool in data mining research and applications. It is the subject of active research in many fields of study, such as computer science, data science, statistics, pattern recognition, artificial intelligence, and machine learning. Several clustering techniques have been proposed and implemented, and most of them successfully find excellent quality or optimal clustering results in the domains mentioned earlier. However, there has been a gradual shift in the choice of clustering methods among domain experts and practitioners alike, which is precipitated by the fact that most traditional clustering algorithms still depend on the number of clusters provided a priori. These conventional clustering algorithms cannot effectively handle real-world data clustering analysis problems where the number of clusters in data objects cannot be easily identified. Also, they cannot effectively manage problems where the optimal number of clusters for a high-dimensional dataset cannot be easily determined. Therefore, there is a need for improved, flexible, and efficient clustering techniques. Recently, a variety of efficient clustering algorithms have been proposed in the literature, and these algorithms produced good results when evaluated on real-world clustering problems. This study presents an up-to-date systematic and comprehensive review of traditional and state-of-the-art clustering techniques for different domains. This survey considers clustering from a more practical perspective. It shows the outstanding role of clustering in various disciplines, such as education, marketing, medicine, biology, and bioinformatics. It also discusses the application of clustering to different fields attracting intensive efforts among the scientific community, such as big data, artificial intelligence, and robotics. This survey paper will be beneficial for both practitioners and researchers. It will serve as a good reference point for researchers and practitioners to design improved and efficient state-of-the-art clustering algorithms.}
}

@article{Joshi2015,
    author = {Kapil Joshi and Himanshu Gupta and Prashant Chaudhary and Punit Sharma},
    title = {Survey on Different Enhanced K-Means Clustering Algorithm},
    journal = {International Journal of Engineering Trends and Technology (IJETT)},
    volume = {27},
    number = {4},
    pages = {178-182},
    year = {2015},
    month = {September},
    issn = {2231-5381},
    doi = {https://dx.doi.org/10.14445/22315381/IJETT-V27P233},
    url = {https://ijettjournal.org/archive/ijett-v27p233},
    publisher = {Seventh Sense Research Group}
}

@article{Morissette2013,
    author = {Morissette, Laurence AND Chartier, Sylvain },
    journal = {Tutorials in Quantitative Methods for Psychology},
    publisher = {TQMP},
    title = {The k-means clustering technique: General considerations and implementation in Mathematica},
    year = {2013},
    volume = {9},
    number = {1},
    url = {http://www.tqmp.org/RegularArticles/vol09-1/p015/p015.pdf },
    pages = {15-24},
    abstract = {Data clustering techniques are valuable tools for researchers working with large databases of multivariate data. In this tutorial, we present a simple yet powerful one: the k-means clustering technique, through three different algorithms: the Forgy/Lloyd, algorithm, the MacQueen algorithm and the Hartigan and Wong algorithm. We then present an implementation in Mathematica and various examples of the different options available to illustrate the application of the technique.},
    doi = {10.20982/tqmp.09.1.p015}
}

@misc{Amidi2018,
    author = {Afshine Amidi and Shervine Amidi},
    title = {Stanford CS 229 Machine Learning Cheatsheets},
    year = {2018},
    url = {https://github.com/afshinea/stanford-cs-229-machine-learning},
    note = {Licensed under MIT License}
}

@article{Bhargav2016,
    author = {Bhargav, Sushant and Pawar, Mahesh},
    year = {2016},
    month = {03},
    pages = {39-44},
    title = {A Review of Clustering Methods forming Non-Convex clusters with, Missing and Noisy Data},
    volume = {3},
    journal = {INTERNATIONAL JOURNAL OF COMPUTER SCIENCES AND ENGINEERING}
}

@dataset{KaggleMallDataset,
    author = {Kaggle},
    title = {Mall Customer Segmentation Data},
    year = {2018},
    url = {https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python},
    note = {[Online; last accessed on March 19, 2025]}
}

@article{FRANTI201995,
    title = {How much can k-means be improved by using better initialization and repeats?},
    journal = {Pattern Recognition},
    volume = {93},
    pages = {95-112},
    year = {2019},
    issn = {0031-3203},
    doi = {https://doi.org/10.1016/j.patcog.2019.04.014},
    url = {https://www.sciencedirect.com/science/article/pii/S0031320319301608},
    author = {Pasi Fränti and Sami Sieranoja},
    keywords = {Clustering algorithms, K-means, Initialization, Clustering accuracy, Prototype selection},
    abstract = {In this paper, we study what are the most important factors that deteriorate the performance of the k-means algorithm, and how much this deterioration can be overcome either by using a better initialization technique, or by repeating (restarting) the algorithm. Our main finding is that when the clusters overlap, k-means can be significantly improved using these two tricks. Simple furthest point heuristic (Maxmin) reduces the number of erroneous clusters from 15% to 6%, on average, with our clustering benchmark. Repeating the algorithm 100 times reduces it further down to 1%. This accuracy is more than enough for most pattern recognition applications. However, when the data has well separated clusters, the performance of k-means depends completely on the goodness of the initialization. Therefore, if high clustering accuracy is needed, a better algorithm should be used instead.}
}

@INPROCEEDINGS{Abdullah10601123,
    author = {İhsanoğlu, Abdullah and Zaval, Mounes},
    booktitle = {2024 32nd Signal Processing and Communications Applications Conference (SIU)},
    title = {Statistically Improving K-means Clustering Performance},
    year = {2024},
    volume = {},
    number = {},
    pages = {1-4},
    keywords = {Signal processing algorithms;Clustering algorithms;Partitioning algorithms;Unsupervised learning;Signal resolution;Unsupervised learning;K-means;K-means++},
    doi = {10.1109/SIU61531.2024.10601123}
}
