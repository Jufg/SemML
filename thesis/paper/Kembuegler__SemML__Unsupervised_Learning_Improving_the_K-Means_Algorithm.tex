\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{semml}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{amsthm}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\semmlfinalcopy % *** Uncomment this line for the final submission

\def\semmlPaperID{****} % *** Enter the SemML Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifsemmlfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Unsupervised Learning: Improving the K-Means Algorithm}

\author{Juri Kembügler\\
Friedrich-Alexander-Universität\\
Erlangen, Germany\\
{\tt\small\href{mailto:juri.kembuegler@fau.de}{juri.kembuegler@fau.de}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    The ABSTRACT is to be in fully-justified italicized text, at the top
    of the left-hand column, below the author and affiliation
    information. Use the word ``Abstract'' as the title, in 12-point
    Times, boldface type, centered relative to the column, initially
    capitalized. The abstract is to be in 10-point, single-spaced type.
    Leave two blank lines after the Abstract, then begin the main text.
    Look at previous SemML abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT

\section{Introduction}\label{sec:introduction}
% TODO: Add sources to the Introduction

Digitalisation has permeated various aspects of our daily lives, from sports
and fitness tracking to entertainment platforms such as YouTube and Twitch, as
well as digital newspaper consumption. A key consequence of this transformation
in recent years is the vast amount of data generated by computers, smartphones,
smartwatches and other IoT devices. In 2010, the global data volume was
%TODO: it was projected?
approximately 2 zettabytes, whereas by 2024, it is projected to reach around
150 zettabytes (1 zettabyte $\approx10^{21}$ bytes). Notably, since the onset
of the global COVID-19 pandemic in 2020, the growth rate of data generation and
consumption has accelerated significantly~\cite{IDC_Statista_2024}. Thus,
extracting meaningful insights from this data requires efficient and robust
algorithms.

To extract meaningful insights from this data, Machine Learning has become an
essential tool. Machine Learning algorithms enable systems to identify patterns
and make decisions based on data. One important branch of Machine Learning is
unsupervised learning, where algorithms uncover hidden structures in data
without predefined labels. A fundamental technique in this domain is
clustering, with K-Means being one of the most widely used methods. However,
the performance of K-Means is highly dependent on the inital choice of
centroids.
% TODO: Source??? And maybe do not mention K-means and unsupervised learning herer?

\subsubsection{Contributions}

In this article, we make the following contributions:
%TODO: should we call this "contributions"?? Maybe

\begin{itemize}
    \item We provide an overview of clustering as an unsupervised Machine Learning
          technique.
    \item We analyze the K-Means algorithm, discussing its methodology and limitations.
    \item We present K-Means++ as a well-known and a statistical relatively new, less
          commonly used approach to improve K-Means.
    \item (Hopefully we also find an improvment for the statistical approach)
    \item We evaluate the performance of these improvements on various datasets.
\end{itemize}

%-------------------------------------------------------------------------

\section{Background and Related Work}\label{sec:background-and-related-work}

%------------------------------------------------------------------------

\subsection{Machine Learning techniques}\label{subsec:machine-learning-techniques}

Machine learning leverages statistical methods, mathematical models, and
numerical techniques to extract meaningful patterns from data, enabling tasks
such as summarization, clustering, visualization, and prediction. The learning
process is generally categorized into two main paradigms: \textit{supervised}
and \textit{unsupervised learning}~\cite{deuschle2019}.

%-------------------------------------------------------------------------
% TODO: Check Plagiat on kushawahAndYadav2016
\subsubsection{Supervised Learning}\label{subsubsec:supervised-learning}

A supervised training technique is the technique where we consider both the
input and the output before using our machine learning model. We then attempt
to use our model to make statements or predictions about new unseen
data~\cite{deuschle2019}. Known techniques are neural networks, several layers
perception and decision trees~\cite{kushawahAndYadav2016}.

%------------------------------------------------------------------------

\subsubsection{Unsupervised Learning}\label{subsubsec:unsupervised-learning}

An unsupervised training technique is the technique where we now only consider
a dataset of only the inputs. We then use our Machine Learining model, to
somewhat describe, summarize or categorize our data~\cite{deuschle2019}. Known
techniques are non-identical types of clustering, amplitude and normalisation,
k-means and self-organising maps~\cite{kushawahAndYadav2016}.

%------------------------------------------------------------------------

\subsection{Clustering}\label{subsec:clustering}

The objective of clustering is to group similar objects based on their
characteristics. The concept of '\textit{similarity}' implies the use of a
distance metric to quantify the relationships between data points. One of the
most widely used distance metrics is the L2 norm (Euclidean
distance)~\cite{deuschle2019}. Alternative metrics such as the L1 norm,
Itakura-Saito or Bregman distance can also be employed, depending on the
specific application and data properties~\cite{Jain2010651}.

Clustering is mainly used to get an overview of data, to summarise it and to
simplify human interpretation by categorising the data and highlighting
characteristics that distinguish the data points. However, it can also be used
as a preprocessing step for other methods (e.g., feature creation for
supervised learning)~\cite{Jain2010651}.

%------------------------------------------------------------------------

\subsubsection{Common clustering algorithms}\label{subsubsec:common-clustering-algorithms}

Overall, clustering algorithms can be classified into two categories:
\textit{hierarchical} and \textit{partitional} algorithms. Hierarchical
algorithms build clusters by recursively merging related data points, whereas
partitional algorithms divide the dataset into distinct clusters
simultaneously~\cite{Ezugwu2022104743,Jain2010651}.

Hierarchical clustering methods include \textit{agglomerative} approaches (such
as single-, complete-, and average-linkage) and \textit{divisive} (e.g.,
monothetic and polythetic) clustering. Partitional techniques, on the other
hand, encompass algorithms such as \textit{K-Means}, \textit{Genetic
    Algorithms}, \textit{Gaussian Mixture Models}, \textit{Fuzzy C-Means}, and
\textit{DBSCAN}~\cite{Ezugwu2022104743}.

%------------------------------------------------------------------------

\subsection{Types of Clusters}\label{subsec:types-of-clusters}

Kapil Joshi \etal~\cite{Joshi2015} divide clusters into four categories.
%TODO: Proof this
In the general literature on cluster analysis, it is discussed that the choice
of the appropriate algorithm depends on the underlying cluster structure
    [further sources]. For example, density-based methods such as DBSCAN are
particularly suitable for non-convex clusters, while k-Means works particularly
well for spherical clusters.

\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsubsection]
\subsubsection{Definitions of cluster types}

\begin{definition}[Well-separated clusters]\label{def:well-separated}
    A point in a cluster is closer or more similar to any point in the same cluster
    than to a point in another cluster~\cite{Joshi2015}.
\end{definition}

\begin{definition}[Centre-based clusters]\label{def:centre-based}
    A point in a cluster is closer or more similar to the ‘centre’ of this cluster
    than to the centre of another cluster~\cite{Joshi2015}.
\end{definition}

\begin{definition}[Contiguous clusters]\label{def:contiguous}
    A point in a cluster is closer to one or more points in the same cluster than to
    any other point not in the cluster~\cite{Joshi2015}.
\end{definition}

\begin{definition}[Density-based clusters]\label{def:density-based}
    A cluster is a dense region of points separated from other high-density
    regions by low-density regions~\cite{Joshi2015}.
\end{definition}

%------------------------------------------------------------------------

\section{K-Means}\label{sec:k-means}

In contrast to other algorithms, K-Means was discovered independently in
various research fields, first by Steinhaus (1956) and Lloyd (proposed 1957;
published 1982) and later by Ball and Hall (1965) and MacQueen
(1967)~\cite{Jain2010651}.

%------------------------------------------------------------------------

\subsection{Procedure and Lloyd's algorithm}\label{subsec:procedure-and-lloyd's-algorithm}

Lorem ipsum odor amet, consectetuer adipiscing elit. Tempor a proin quam cursus
tincidunt id fringilla. Netus hac suscipit maximus hendrerit luctus massa.
Hendrerit mollis nibh curabitur magnis netus ad diam. Sapien per conubia id
primis auctor. Porta metus feugiat tincidunt amet suscipit habitasse.

%------------------------------------------------------------------------

\subsection{Problems and limitations}\label{subsec:problems-and-limitations}

Lorem ipsum odor amet, consectetuer adipiscing elit. Tempor a proin quam cursus
tincidunt id fringilla. Netus hac suscipit maximus hendrerit luctus massa.
Hendrerit mollis nibh curabitur magnis netus ad diam. Sapien per conubia id
primis auctor. Porta metus feugiat tincidunt amet suscipit habitasse.

%------------------------------------------------------------------------

\subsection{K-Means++}\label{subsec:k-means++}

Lorem ipsum odor amet, consectetuer adipiscing elit. Tempor a proin quam cursus
tincidunt id fringilla. Netus hac suscipit maximus hendrerit luctus massa.
Hendrerit mollis nibh curabitur magnis netus ad diam. Sapien per conubia id
primis auctor. Porta metus feugiat tincidunt amet suscipit habitasse.

%------------------------------------------------------------------------

\section{Statistically improving K-Means}\label{sec:statistically-improving-k-means}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
culpa qui officia deserunt mollit anim id est laborum.

%------------------------------------------------------------------------

\subsection{Identifying issues}\label{subsec:identifying-issues}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
culpa qui officia deserunt mollit anim id est laborum.

%------------------------------------------------------------------------

\subsection{The improved algorithm}\label{subsec:the-improved-algorithm}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
culpa qui officia deserunt mollit anim id est laborum.

%------------------------------------------------------------------------

\subsection{Challenges}\label{subsec:challenges}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
culpa qui officia deserunt mollit anim id est laborum.

%------------------------------------------------------------------------

\section{Results and Performance Evaluation}\label{sec:results-and-performance-evaluation}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
culpa qui officia deserunt mollit anim id est laborum.

%------------------------------------------------------------------------

\subsection{Metrics}

Lorem ipsum odor amet, consectetuer adipiscing elit. Tempor a proin quam cursus
tincidunt id fringilla. Netus hac suscipit maximus hendrerit luctus massa.
Hendrerit mollis nibh curabitur magnis netus ad diam. Sapien per conubia id
primis auctor. Porta metus feugiat tincidunt amet suscipit habitasse.

%------------------------------------------------------------------------

\subsection{Case Study: Mall Customer Segmentation}

Lorem ipsum odor amet, consectetuer adipiscing elit. Tempor a proin quam cursus
tincidunt id fringilla. Netus hac suscipit maximus hendrerit luctus massa.
Hendrerit mollis nibh curabitur magnis netus ad diam. Sapien per conubia id
primis auctor. Porta metus feugiat tincidunt amet suscipit habitasse.

%------------------------------------------------------------------------

\subsection{Measurements on different datasets}

Lorem ipsum odor amet, consectetuer adipiscing elit. Tempor a proin quam cursus
tincidunt id fringilla. Netus hac suscipit maximus hendrerit luctus massa.
Hendrerit mollis nibh curabitur magnis netus ad diam. Sapien per conubia id
primis auctor. Porta metus feugiat tincidunt amet suscipit habitasse.

%------------------------------------------------------------------------

\section{Conclusion and Future Work}\label{sec:conclusion-and-future-work}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
culpa qui officia deserunt mollit anim id est laborum.

%------------------------------------------------------------------------

\subsection{Summary}

Lorem ipsum odor amet, consectetuer adipiscing elit. Tempor a proin quam cursus
tincidunt id fringilla. Netus hac suscipit maximus hendrerit luctus massa.
Hendrerit mollis nibh curabitur magnis netus ad diam. Sapien per conubia id
primis auctor. Porta metus feugiat tincidunt amet suscipit habitasse.

    %------------------------------------------------------------------------

    {\small
        \bibliographystyle{ieee}
        \bibliography{kembuegler_bib}
    }

\end{document}
